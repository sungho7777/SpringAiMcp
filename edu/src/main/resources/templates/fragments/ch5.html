<!DOCTYPE html>
    <html lang="ko" xmlns:th="http://www.thymeleaf.org">

    <th:block>

        <h1>Chapter 5. Multimodality API – Audio and Speech</h1><br>

        <h2>1. Generating with Voice</h2>

        <p>
            인간은 시각, 청각, 언어 등 다양한 감각 채널을 동시에 활용하여 정보를 이해하고 학습합니다.
            그러나 전통적인 머신 러닝 시스템은 특정 입력 형식에 최적화된
            <strong>단일 모달리티(Single-Modality) 모델</strong> 중심으로 발전해 왔습니다.
            텍스트-음성 변환(TTS), 음성-텍스트 변환(STT)을 수행하는 오디오 모델,
            그리고 객체 인식·분류를 위한 컴퓨터 비전 모델이 대표적인 사례입니다.
        </p>

        <p>
            최근에는 서로 다른 입력 형식을 통합적으로 처리할 수 있는
            <strong>멀티모달 대규모 언어 모델(MLLM: Multimodal Large Language Model)</strong>이 등장하여
            텍스트뿐 아니라 이미지, 오디오, 비디오 등 다양한 데이터를 동시에 받아들이고,
            이를 종합적으로 분석하여 고품질의 텍스트 기반 응답을 생성할 수 있게 되었습니다.
        </p>

        <p>대표적인 멀티모달 모델은 다음과 같습니다:</p>

        <ul>
            <li><strong>OpenAI GPT-4o</strong></li>
            <li><strong>Google Vertex AI Gemini 1.5</strong></li>
            <li><strong>Anthropic Claude 3</strong></li>
            <li><strong>오픈소스 모델:</strong> Llama3.2, LLaVA, BakLLaVA 등</li>
        </ul>

        <p>
            이러한 모델들은 다양한 소스의 입력을 병렬로 처리하고 의미 구조를 통합하여 추론할 수 있는
            <strong>멀티모달리티(Multimodality)</strong> 능력을 기반으로 합니다.
            Spring AI에서 제공하는 <strong>Message API</strong>는 이러한 멀티모달 LLM을 자연스럽게 활용할 수 있도록
            필요한 메시지 구조 및 추상화를 제공합니다.
        </p>
        <hr>

        <h2>2. UserMessage와 Media 필드 구조</h2>
        <img th:src="@{/imgs/multimodality.png}" width="600px;"><br><a href="https://docs.spring.io/spring-ai/reference/api/multimodality.html">출처:docs.springai.io</a><br><br>

        <p>
            Spring AI의 <code>UserMessage</code>는 기본적으로 텍스트 데이터를 전달하기 위한
            <strong>content</strong> 필드를 중심으로 구성됩니다.
            하지만 선택적으로 <strong>media</strong> 필드를 추가함으로써,
            이미지(Image), 오디오(Audio), 비디오(Video) 등 다양한 멀티모달 입력을 함께 전송할 수 있습니다.
        </p>

        <p>
            <strong>media</strong> 필드에는 해당 데이터의 형식을 지정하는 <strong>MimeType</strong>이 포함되며,
            사용하는 LLM의 요구사항에 따라 미디어 데이터는 다음 두 가지 형태로 제공됩니다.
        </p>

        <ul>
            <li><strong>Resource 기반 원시(raw) 미디어 데이터</strong></li>
            <li><strong>URI 기반 외부 리소스 참조</strong></li>
        </ul>

        <p>
            이 구조는 모델별 입력 포맷 차이를 유연하게 처리할 수 있도록 설계되어 있으며,
            멀티모달 데이터를 결합하는 고급 AI 상호작용을 애플리케이션에 자연스럽게 포함할 수 있게 합니다.
        </p>
        <hr>

        <h2>3. Spring AI에서 지원하는 Multimodal Chat Models</h2>

        <p>Spring AI는 다음과 같은 주요 멀티모달 모델과의 연동을 공식적으로 지원합니다:</p>

        <ul>
            <li><strong>Anthropic – Claude 3</strong></li>
            <li><strong>AWS Bedrock – Converse (Multimodal 지원)</strong></li>
            <li><strong>Azure OpenAI – GPT-4o 계열</strong></li>
            <li><strong>Mistral AI – Pixtral 모델</strong></li>
            <li><strong>Ollama – LLaVA, BakLLaVA, Llama3.2 모델</strong></li>
            <li><strong>OpenAI – GPT-4, GPT-4o</strong></li>
            <li><strong>Google Vertex AI – Gemini 1.5 Pro 및 Flash</strong></li>
        </ul>

        <p>
            Spring AI를 활용하면 텍스트 기반 상호작용뿐 아니라 음성 입력·음성 생성·오디오 분석 등
            다양한 멀티모달 기능을 애플리케이션 환경에 손쉽게 통합할 수 있습니다.
        </p>
        <hr>

        <h2>4. Text to Speech</h2><br>
        <a href="https://platform.openai.com/docs/guides/text-to-speech">출처:platform.openai.com</a><br><br>
        <pre style="border:1px solid cornflowerblue">

            public Ch5_OpenAiAudioService(ChatClient.Builder chatClientBuilder,
                                          OpenAiAudioTranscriptionModel openAiAudioTranscriptionModel,
                                          OpenAiAudioSpeechModel openAiAudioSpeechModel) {
                chatClient = chatClientBuilder.build();
                this.openAiAudioTranscriptionModel = openAiAudioTranscriptionModel;
                this.openAiAudioSpeechModel = openAiAudioSpeechModel;
                this.speechOptions = OpenAiAudioSpeechOptions.builder()
                        .model("gpt-4o-mini-tts")
                        .voice(OpenAiAudioApi.SpeechRequest.Voice.NOVA)
                        .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3)
                        .speed(1.0)
                        .build();;
            }

            public Map&lt;String, String&gt; textToSpeech(String text) {

                TextToSpeechPrompt speechPrompt = new TextToSpeechPrompt(text, speechOptions);
                TextToSpeechResponse response = openAiAudioSpeechModel.call(speechPrompt);
                byte[] bytes = response.getResult().getOutput();

                String base64Audio = Base64.getEncoder().encodeToString(bytes);
                Map&lt;String, String&gt; result = new HashMap&lt;&gt;();
                result.put("audio", base64Audio);

                return result;
            }

        </pre><br>
        <hr>

        <h2>5. Speech to Text</h2><br>
        <a href="https://platform.openai.com/docs/guides/speech-to-text">출처:platform.openai.com</a><br><br>
        <pre style="border:1px solid cornflowerblue">

            public String speechToText(MultipartFile multipartFile) throws IOException {

                Path tempFile = Files.createTempFile("multipart-", multipartFile.getOriginalFilename());
                multipartFile.transferTo(tempFile);
                Resource audioResource = new FileSystemResource(tempFile);

                OpenAiAudioTranscriptionOptions options = OpenAiAudioTranscriptionOptions.builder()
                        .model("whisper-1")
                        .language("ko") // 입력 음성 언어의 종류 설정, 출력 언어에도 영향을 미침
                        .build();

                AudioTranscriptionPrompt prompt = new AudioTranscriptionPrompt(audioResource, options);

                AudioTranscriptionResponse response = openAiAudioTranscriptionModel.call(prompt);
                String text = response.getResult().getOutput();
                log.info(text);

                return text;
            }
    </pre>
    </th:block>

</html>